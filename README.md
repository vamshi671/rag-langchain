# ğŸ§  RAG Chat Assistant (Ollama + LLaMA 3)

A **Retrieval-Augmented Generation (RAG)** application that allows users to **upload PDF documents** and ask questions grounded **strictly in the document content**.

ğŸ”’ Fully local & private  
âš¡ Powered by **Ollama + LLaMA 3**  

---

## ğŸš€ Features

- ğŸ“„ Upload PDF documents
- ğŸ’¬ Chat-style question answering
- ğŸ“š Answers grounded only in uploaded documents
- ğŸ” Source references with page numbers
- ğŸ–¥ï¸ Clean Streamlit UI
- ğŸ“´ Fully offline (no paid APIs)

---

## ğŸ› ï¸ Tech Stack

- Python
- Streamlit
- LangChain
- FAISS
- Sentence Transformers
- Ollama (LLaMA 3)

---

## ğŸ“‚ Project Structure
```text
rag-langchain/
â”‚
â”œâ”€â”€ data/                # Sample PDFs
â”œâ”€â”€ demo/                # Screenshots / demo images
â”œâ”€â”€ rag/                 # RAG pipeline modules
â”‚   â”œâ”€â”€ embeddings.py
â”‚   â”œâ”€â”€ loader.py
â”‚   â”œâ”€â”€ splitter.py
â”‚   â”œâ”€â”€ vectorstore.py
â”‚   â””â”€â”€ qa_chain.py
â”‚
â”œâ”€â”€ app.py               # Streamlit app
â”œâ”€â”€ main.py              # Optional CLI / testing
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ Dockerfile
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md

### 1ï¸âƒ£ Install Ollama
```bash
brew install ollama
ollama pull llama3
ollama serve
```
2ï¸âƒ£ Clone the Repository
```bash
git clone https://github.com/vamshi671/rag-langchain.git
cd rag-langchain
```
3ï¸âƒ£ Create Virtual Environment
```bash
python -m venv rag-env
source rag-env/bin/activate
pip install -r requirements.txt
```
4ï¸âƒ£ Run the Application'
```bash
streamlit run app.py
```
ğŸ³  Docker
```bash
docker build -t rag-chat-app .
docker run -p 8501:8501 rag-chat-app
```






